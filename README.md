🧠 Artificial Neural Network Experiments (ANN3)

This repository contains implementations of fundamental Artificial Neural Network (ANN) algorithms and concepts.
Each experiment demonstrates an important principle in neural computation — from simple perceptrons to advanced self-organizing models.

📋 List of Experiments
Experiment 3 – Perceptron Model for Logical Functions

Implemented the Perceptron algorithm to simulate basic logical operations:

Logical AND

Logical OR

Logical NOT

📘 Concept:
A perceptron is the simplest form of a neural network — a single-layer model that makes predictions using a weighted sum and activation function.

Experiment 4 – Plot Activation Functions

Plotted commonly used activation functions in neural networks:

Step Function

Sigmoid Function

Tanh Function

ReLU Function

Leaky ReLU

📘 Concept:
Activation functions introduce non-linearity into neural networks, enabling them to learn complex mappings.

Experiment 5 – Train a Feed Forward Neural Network

Implemented and trained a Feed Forward Neural Network (FFNN) on sample data.

📘 Concept:
FFNNs pass information forward — from input to output — and are trained using algorithms like gradient descent.

Experiment 6 – Hebb’s Learning Rule

Implemented Hebb’s Rule for basic logical functions like AND & OR.

📘 Concept:
Hebbian learning is based on the principle — “Neurons that fire together, wire together.”
Weights are updated when input and output neurons are simultaneously active.

Experiment 7 – Backpropagation in Feed Forward Neural Network

Trained a Feed Forward Neural Network using the Backpropagation Algorithm.

📘 Concept:
Backpropagation minimizes prediction error by adjusting weights in proportion to their contribution to the error — a key step in deep learning.

Experiment 8 – Self Organizing Map (SOM)

Implemented a Kohonen Self Organizing Map for unsupervised clustering.

📘 Concept:
SOMs map high-dimensional input data to lower-dimensional (usually 2D) grids, preserving topological relationships.

Experiment 9 – Learning Vector Quantization (LVQ)

Implemented the Learning Vector Quantization (LVQ) algorithm.

📘 Concept:
LVQ is a supervised version of SOM — used for pattern classification by fine-tuning prototype vectors for each class.

⚙️ Technologies Used

Python 🐍

NumPy

Matplotlib

Scikit-learn (for some models)

📊 Output Samples

Each experiment includes:

Source Code (.py files or notebooks)

Output Plots / Tables

Explanations and comments
