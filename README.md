# ğŸ§  Artificial Neural Network Experiments

This repository contains implementations of fundamental **Artificial Neural Network (ANN)** algorithms and concepts.  
Each experiment demonstrates a key concept â€” from simple perceptrons to advanced self-organizing models.

---

## ğŸ“‹ List of Experiments

### âš™ï¸ **Experiment 3 â€“ Perceptron Model for Logical Functions**
Implemented the **Perceptron algorithm** to simulate basic logical operations:
- ğŸ”¸ Logical AND  
- ğŸ”¸ Logical OR  
- ğŸ”¸ Logical NOT  

**Concept:**  
A perceptron is the simplest neural network model â€” a single-layer neuron that computes a weighted sum and applies an activation function to make predictions.

---

### ğŸ“ˆ **Experiment 4 â€“ Plot Various Activation Functions**
Plotted the most commonly used **activation functions** in neural networks:
- Step Function  
- Sigmoid Function  
- Tanh Function  
- ReLU Function  
- Leaky ReLU Function  

**Concept:**  
Activation functions introduce **non-linearity**, enabling neural networks to learn complex and real-world relationships.

---

### ğŸ§© **Experiment 5 â€“ Train a Feed Forward Neural Network**
Developed and trained a **Feed Forward Neural Network (FFNN)** on sample data.

**Concept:**  
In a feed-forward network, information moves in one direction â€” from input to output â€” and learning occurs using **gradient descent**.

---

### ğŸ”— **Experiment 6 â€“ Hebbâ€™s Learning Rule**
Implemented **Hebbâ€™s Rule** for basic logical functions like AND and OR.

**Concept:**  
Hebbian learning follows the principle â€” *â€œNeurons that fire together, wire together.â€*  
Weights are updated when input and output neurons are simultaneously active.

---

### ğŸ” **Experiment 7 â€“ Backpropagation in Feed Forward Neural Network**
Trained a **Feed Forward Neural Network** using the **Backpropagation algorithm**.

**Concept:**  
Backpropagation adjusts network weights in proportion to their contribution to the output error â€” forming the foundation of modern **deep learning**.

---

### ğŸ§­ **Experiment 8 â€“ Self-Organizing Map (SOM)**
Implemented the **Kohonen Self-Organizing Map (SOM)** for **unsupervised clustering**.

**Concept:**  
SOMs project high-dimensional input data into a **2D grid** while preserving topological relationships between data points.

---

### ğŸ¯ **Experiment 9 â€“ Learning Vector Quantization (LVQ)**
Implemented the **Learning Vector Quantization (LVQ)** algorithm.

**Concept:**  
LVQ is a **supervised learning** technique derived from SOM, used for pattern classification by fine-tuning class prototype vectors.

---

## ğŸ’» Technologies Used
- Python ğŸ  
- NumPy  
- Matplotlib  
- Scikit-learn (for some experiments)

---

## ğŸ“Š Output and Results
Each experiment includes:
- âœ… Source Code (`.py` / `.ipynb`)  
- ğŸ“ˆ Output Plots and Tables  
- ğŸ§  Concept Explanation and Comments  

---

## ğŸš€ How to Run

1. **Clone the repository**
   ```bash
   git clone https://github.com/<your-username>/ANN_LAB.git

2. Navigate to the project folder
```
cd ANN_LAB
