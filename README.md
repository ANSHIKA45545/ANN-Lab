ğŸ§  Artificial Neural Network Experiments (ANN3)

This repository contains implementations of fundamental Artificial Neural Network (ANN) algorithms and concepts.
Each experiment demonstrates an important principle in neural computation â€” from simple perceptrons to advanced self-organizing models.

ğŸ“‹ List of Experiments
Experiment 3 â€“ Perceptron Model for Logical Functions

Implemented the Perceptron algorithm to simulate basic logical operations:

Logical AND

Logical OR

Logical NOT

ğŸ“˜ Concept:
A perceptron is the simplest form of a neural network â€” a single-layer model that makes predictions using a weighted sum and activation function.

Experiment 4 â€“ Plot Activation Functions

Plotted commonly used activation functions in neural networks:

Step Function

Sigmoid Function

Tanh Function

ReLU Function

Leaky ReLU

ğŸ“˜ Concept:
Activation functions introduce non-linearity into neural networks, enabling them to learn complex mappings.

Experiment 5 â€“ Train a Feed Forward Neural Network

Implemented and trained a Feed Forward Neural Network (FFNN) on sample data.

ğŸ“˜ Concept:
FFNNs pass information forward â€” from input to output â€” and are trained using algorithms like gradient descent.

Experiment 6 â€“ Hebbâ€™s Learning Rule

Implemented Hebbâ€™s Rule for basic logical functions like AND & OR.

ğŸ“˜ Concept:
Hebbian learning is based on the principle â€” â€œNeurons that fire together, wire together.â€
Weights are updated when input and output neurons are simultaneously active.

Experiment 7 â€“ Backpropagation in Feed Forward Neural Network

Trained a Feed Forward Neural Network using the Backpropagation Algorithm.

ğŸ“˜ Concept:
Backpropagation minimizes prediction error by adjusting weights in proportion to their contribution to the error â€” a key step in deep learning.

Experiment 8 â€“ Self Organizing Map (SOM)

Implemented a Kohonen Self Organizing Map for unsupervised clustering.

ğŸ“˜ Concept:
SOMs map high-dimensional input data to lower-dimensional (usually 2D) grids, preserving topological relationships.

Experiment 9 â€“ Learning Vector Quantization (LVQ)

Implemented the Learning Vector Quantization (LVQ) algorithm.

ğŸ“˜ Concept:
LVQ is a supervised version of SOM â€” used for pattern classification by fine-tuning prototype vectors for each class.

âš™ï¸ Technologies Used

Python ğŸ

NumPy

Matplotlib

Scikit-learn (for some models)

ğŸ“Š Output Samples

Each experiment includes:

Source Code (.py files or notebooks)

Output Plots / Tables

Explanations and comments
